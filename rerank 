# src/search_steps/impls/reranking/CrossEncoder.py
from __future__ import annotations

import json
import re
from pathlib import Path
from typing import Dict, Iterable, List, Tuple

from sentence_transformers import CrossEncoder
from src.search_steps.impls.reranking.CrerankingABS import Reranking
from src.pipeline.Context import Context
from src.utils.logging import get_logger


class CrossEncoderReranking(Reranking):
    name = "cross_encoder_rerank"

    # ---------------- helpers ----------------
    def _load_model(self, model_name: str) -> CrossEncoder:
        if CrossEncoder is None:
            raise RuntimeError("sentence-transformers not available.")
        print(f"Loading cross-encoder: {model_name}")
        return CrossEncoder(model_name, max_length=512)  # keep softmax=False for BGE

    @staticmethod
    def _clean(text: str) -> str:
        """Fix PDF hyphenation and collapse ugly newlines while keeping paragraphs."""
        t = text.replace("-\n", "")
        t = t.replace("\r", "\n")
        t = "\n".join(line.strip() for line in t.splitlines())
        t = "\n\n".join(p for p in t.split("\n\n") if p.strip())
        return t

    @staticmethod
    def _windows(text: str, approx_tokens: int) -> List[str]:
        """Split into sentence-ish windows ~approx_tokens words with small overlap."""
        sents = re.split(r"(?<=[.!?:;])\s+", text)
        blocks, cur, wc = [], [], 0
        for s in sents:
            cur.append(s)
            wc += len(s.split())
            if wc >= approx_tokens:
                blocks.append(" ".join(cur))
                # keep last sentence as overlap
                cur, wc = cur[-1:], len(cur[-1].split())
        if cur:
            blocks.append(" ".join(cur))
        return blocks or [text]

    @staticmethod
    def _de_query_variants(q: str) -> List[str]:
        """Tiny German compound expansion; we aggregate by MAX so precision is kept."""
        variants = [q]
        ql = q.lower()
        extra: List[str] = []
        if "querprofil" in ql:
            extra += ["Querprofillinie", "Querprofillinien", "Querschnittsprofil", "Querprofil-Linie"]
        if "station" in ql or "achsstation" in ql:
            extra += ["Achsstation", "Stationierung", "Stationen"]
        if "plot" in ql or "plotten" in ql:
            extra += ["zeichnen", "darstellen", "plot", "erstellen"]
        if extra:
            variants.append(q + " (" + " ".join(extra[:6]) + ")")
        return variants

    @staticmethod
    def _norm(a: Iterable[float]) -> List[float]:
        a = list(float(x) for x in a)
        if not a:
            return a
        mn, mx = min(a), max(a)
        rng = mx - mn
        return [(x - mn) / (rng + 1e-9) for x in a]

    # ---------------- main ----------------
    def run(self, ctx: "Context") -> None:
        log = get_logger(self.name)

        cfg = ctx.cfg
        paths = cfg["paths"]
        hybrid_json_path = Path(paths["hybrid_results_path"])

        # config knobs (with safe defaults)
        rerank_cfg: Dict[str, object] = cfg.get("rerank", {})
        model_name = str(rerank_cfg.get("model_name", "BAAI/bge-reranker-v2-m3"))
        top_k = int(rerank_cfg.get("top_k", 10))
        window_tokens = int(rerank_cfg.get("window_tokens", 180))
        alpha = float(rerank_cfg.get("alpha", 0.65))  # CE weight vs retriever

        # query source (keep your previous fallback order)
        query = (
            ctx.artifacts.get("corrected_query")
            or cfg.get("query", {}).get("text", "")
            or ""
        )
        if not query.strip():
            raise ValueError("cfg.query.text is required for cross-encoder rerank.")

        log.info(f"Using query: {query}")
        log.info(f"Loading results from: {hybrid_json_path}")

        # load hybrid results
        with hybrid_json_path.open("r", encoding="utf-8") as f:
            data = json.load(f)
        raw_results: List[Dict] = list(data.get("results", []))
        if not raw_results:
            log.warning("No results to rerank; exiting.")
            return

        # load CE
        model = self._load_model(model_name)

        # collect candidates and their retriever scores (prefer RRF)
        candidates: List[Tuple[str, str]] = []
        retriever_scores: List[float] = []
        for r in raw_results:
            chunk_path = Path(r.get("chunkpath", ""))
            if not chunk_path.exists():
                log.warning(f"Chunk file not found: {chunk_path}")
                continue
            text = chunk_path.read_text(encoding="utf-8", errors="ignore").strip()
            candidates.append((str(chunk_path), text))
            retriever_scores.append(float(r.get("rrf_score", 0.0)))

        if not candidates:
            log.warning("No readable chunks; exiting.")
            return

        # score each chunk: clean -> windows -> (query variants x windows) -> MAX
        ce_scores: List[float] = []
        qvars = self._de_query_variants(query)

        for i, (path, text) in enumerate(candidates, 1):
            wt = self._windows(self._clean(text), approx_tokens=window_tokens)
            pairs = [(qv, w) for qv in qvars for w in wt]
            scores = model.predict(pairs, batch_size=32)  # logits; softmax=False
            ce_scores.append(float(max(scores)))
            if i % 10 == 0 or i == len(candidates):
                log.info(f"Scored {i}/{len(candidates)} with CE")

        # blend with retriever score to preserve exact matches (esp. German compounds)
        ce_n = self._norm(ce_scores)
        ret_n = self._norm(retriever_scores)
        blended = [alpha * c + (1.0 - alpha) * r for c, r in zip(ce_n, ret_n)]

        # sort and print top-k
        order = sorted(range(len(candidates)), key=lambda i: blended[i], reverse=True)
        log.info("Top 10 chunks based on reranking:")
        for rank, i in enumerate(order[:max(10, top_k)], 1):
            log.info(
                f"Rank {rank}: {candidates[i][0]}  "
                f"(CE: {ce_scores[i]:.6f}, blended: {blended[i]:.6f})"
            )

        # write a reranked results file next to the hybrid one
        out = {
            "query": query,
            "model": model_name,
            "alpha": alpha,
            "results": [
                {
                    "chunkpath": candidates[i][0],
                    "ce_score": ce_scores[i],
                    "blended_score": blended[i],
                    "rrf_score": retriever_scores[i],
                }
                for i in order
            ],
        }
        out_path = hybrid_json_path.with_name("reranked_results.json")
        out_path.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
        log.info(f"Wrote: {out_path}")