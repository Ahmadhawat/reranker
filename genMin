# CllmAnswerMinimal.py
from __future__ import annotations
import requests
from pathlib import Path
from typing import List

TEMPERATURE = 0.2
MAX_TOKENS  = 800
MAX_CTX_CHARS = 18000           # hard cap for all file text sent to LLM
K_FILES = 20                    # max files from copy_dir

SYSTEM_PROMPT = (
    "Du bist ein technischer Assistent. "
    "Beantworte die Frage NUR basierend auf den bereitgestellten Dateien. "
    "Wenn Info fehlt, sag das klar."
)

def _read_text_safe(p: Path) -> str:
    try:
        return p.read_text(encoding="utf-8")
    except Exception:
        return p.read_text(encoding="utf-8", errors="ignore")

def _strip_provenance_header(text: str) -> str:
    if "PROVENANCE-SOURCE-PATH" in text[:500]:
        lines = text.splitlines()
        return "\n".join(lines[1:]) if lines else text
    return text

def _gather_files(copy_dir: str, k: int) -> List[Path]:
    d = Path(copy_dir)
    files = [p for p in d.iterdir() if p.is_file() and p.suffix.lower() not in {".json", ".png", ".jpg", ".jpeg"}]
    files = [p for p in files if p.name != "manifest.json"]
    files.sort()
    return files[:k]

def _truncate(s: str, limit: int) -> str:
    if len(s) <= limit:
        return s
    head = s[: int(limit * 0.8)]
    tail = s[-int(limit * 0.15):]
    return head + "\n...\n" + tail

def _build_user_prompt(query: str, file_blocks: str) -> str:
    return (
        f"Frage: {query}\n\n"
        "Du bekommst Dateien. Antworte kurz, korrekt, mit Schrittfolge wenn sinnvoll.\n\n"
        f"Dateien:\n{file_blocks}\n"
    )

def _call_llm(api_url: str, api_key: str, model: str, system: str, user: str) -> str:
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system},
            {"role": "user",   "content": user},
        ],
        "temperature": TEMPERATURE,
        "max_tokens": MAX_TOKENS,
    }
    resp = requests.post(api_url, headers=headers, json=payload, timeout=60)
    if resp.status_code != 200:
        print(f"[HTTP ERROR] {resp.status_code}: {resp.text[:200]}")
        return ""
    data = resp.json()
    return (data.get("choices", [{}])[0].get("message", {}).get("content") or "").strip()

class LLMAnswerMinimal:
    """
    Usage:
      step = LLMAnswerMinimal()
      step.run(ctx)  # prints the answer
    Expects in ctx.cfg:
      paths.copy_dir
      query.text
      query.API_URL, query.API_KEY, query.MODEL_NAME
      (optional) answer.k, answer.max_ctx_chars
    """
    name = "LLM_Answer_Minimal"

    def run(self, ctx) -> None:
        copy_dir      = ctx.cfg["paths"]["copy_dir"]
        query_text    = (ctx.cfg.get("answer", {}).get("query_text")
                         or ctx.cfg.get("query", {}).get("text") or "").strip()
        api_url       = ctx.cfg["query"]["API_URL"]
        api_key       = ctx.cfg["query"]["API_KEY"]
        model_name    = ctx.cfg["query"]["MODEL_NAME"]
        k             = int(ctx.cfg.get("answer", {}).get("k", K_FILES))
        max_ctx_chars = int(ctx.cfg.get("answer", {}).get("max_ctx_chars", MAX_CTX_CHARS))

        if not (query_text and api_url and api_key and model_name):
            print("[CONFIG ERROR] Missing query or API settings.")
            return

        files = _gather_files(copy_dir, k)
        if not files:
            print("[DATA] No files in copy_dir.")
        
        # build file blocks with global budget
        remaining = max_ctx_chars
        blocks = []
        for i, p in enumerate(files, start=1):
            raw = _strip_provenance_header(_read_text_safe(p))
            header = f"\n[{i}] {p.name}\n---\n"
            budget_for_this = max(0, remaining - len(header) - 200)  # 200 chars safety
            if budget_for_this <= 0:
                break
            body = _truncate(raw, budget_for_this)
            block = header + body
            blocks.append(block)
            remaining -= len(block)

        file_blocks = "".join(blocks).strip()
        user_prompt = _build_user_prompt(query_text, file_blocks if file_blocks else "(keine Dateien)")

        answer = _call_llm(api_url, api_key, model_name, SYSTEM_PROMPT, user_prompt)
        if answer:
            print("\n===== ANSWER =====\n")
            print(answer)
        else:
            print("[LLM] Empty answer.")